End-to-End Azure Data Platform â€“ Final Report

Course: CST89xx  â€¢ Project Length: 14 weeks  â€¢ Group: <Group # / Members>

Title: 

Author(s): 
Date: <Month DD, 2025>

â¸»

0. Executive Summary (Â½â€“1 page)

A concise overview of the business problem, data sources, target users, and the solution you built on Azure. Summarize outcomes (e.g., latency achieved, data freshness, analytics delivered, cost tier).
	â€¢	Business objective: <What decision/outcome are we enabling?>
	â€¢	Data sources: <Kaggle dataset(s), streaming source(s)>
	â€¢	Core platform: Azure (ADF, ADLS Gen2, Databricks/Synapse, Event Hubs, Stream Analytics, Cognitive Services, Warehouse)
	â€¢	Key results: <SLAs, dashboard KPIs, model accuracy, cost/month>

â¸»

1. Use Case (1â€“2 pages)

1.1 Problem Statement

Describe the domain and pain points. Why is a data platform required now? What decisions depend on this platform?

1.2 Stakeholders & Users
	â€¢	Business stakeholders: <e.g., Operations, Marketing>
	â€¢	Data consumers: <Analysts, Data Scientists, Apps>
	â€¢	Operational owners: <Data Engineering, Cloud Ops>

1.3 Success Criteria & KPIs

List measurable targets (e.g., daily ingestion volume, time-to-dashboard < 15 min, query latency < 5 sec, model F1 â‰¥ 0.80).

1.4 Datasets (Kaggle)
	â€¢	Dataset name & link: <Kaggle dataset(s)>
	â€¢	Schema summary: <tables, columns, granularity>
	â€¢	Data volume: <rows/GB>
	â€¢	Data freshness: <batch cadence / streaming>
	â€¢	Data sensitivity: <PII? compliance needs?>

Note: Visualizations donâ€™t count toward page length. Include a table or small ERD of the core dataset.

â¸»

2. Reference Architecture & Services (2â€“3 pages)

2.1 Reference Architecture Narrative

Explain the endâ€‘toâ€‘end flow from ingestion to consumption. Include batch + streaming paths, medallion layers, and security boundaries.

flowchart LR
  subgraph Source[Sources]
    K[Kaggle Batch Files]
    API[External API (optional)]
    IOT[Event Producers]
  end

  K -->|Batch copy| ADF((Azure Data Factory))
  API --> ADF
  IOT --> EH[(Event Hubs)] --> ASA((Stream Analytics))

  ADF --> ADLS[(ADLS Gen2 Bronze)]
  ASA --> ADLS

  ADLS --> DBX((Databricks / Synapse Spark))
  DBX --> ADLS_Silver[(ADLS Gen2 Silver)]
  DBX --> ADLS_Gold[(ADLS Gen2 Gold)]

  ADLS_Gold --> WH[(Synapse Dedicated SQL Pool / Fabric Warehouse)]
  ADLS_Gold --> PBI[(Power BI / Fabric Lakehouse)]

  ADLS_Silver --> AI[(Azure Cognitive Services / Azure ML)]
  AI --> WH

  WH --> BI[Dashboards & Reports]

2.2 Azure Services Map & Rationale

Capability	Service	Why this service	Alternatives
Object storage, data lake	Azure Data Lake Storage Gen2	Hierarchical namespaces, ACLs, low-cost at scale	Blob Storage (basic), Fabric OneLake
Batch orchestration	Azure Data Factory	GUI pipelines, Mapping Data Flows, triggers, integration runtime	Synapse Pipelines, Databricks Jobs
Streaming ingestion	Azure Event Hubs	High-throughput event ingestion	Kafka on HDInsight / Confluent
Stream processing	Azure Stream Analytics	SQL-like streaming queries, easy BI output	Spark Structured Streaming
Transform/ML (big data)	Databricks or Synapse Spark	Distributed compute, notebooks, Delta Lake	Fabric Lakehouse
Warehouse (SQL)	Synapse Dedicated SQL Pool / Fabric Warehouse	MPP analytics, Tâ€‘SQL access	Azure SQL DB Hyperscale
AI addâ€‘on	Azure AI Language / Vision	No-ML ops, prebuilt models for text/image	Azure ML custom models
Monitoring	Log Analytics + Azure Monitor	Central logs/metrics, KQL	Datadog, Grafana
Security	Entra ID, Key Vault, Defender for Cloud	Identity, secrets, posture management	â€”

2.3 Data Modeling & Zones

Describe Bronze â†’ Silver â†’ Gold layers, partitioning strategy, Delta format, Slowly Changing Dimensions (if any), and semantic models for BI.

â¸»

3. Data Warehousing Design (1â€“2 pages)

3.1 Schema Design
	â€¢	Model type: Star schema / Data Vault / Lakehouse tables
	â€¢	Fact tables: <grain, measures>
	â€¢	Dimensions: <conformed dims, SCD approach>

3.2 Performance & Cost Optimizations
	â€¢	Partitioning & Zâ€‘ordering
	â€¢	Materialized views in Synapse / Fabric
	â€¢	Caching and result set reuse
	â€¢	Autoscale settings & workload management

â¸»

4. Big Data Transformations & Advanced Analytics (2â€“3 pages)

4.1 Mapping Data Flows (ADF)
	â€¢	Source/sink datasets, schema drift handling
	â€¢	Derived columns, joins, aggregates, window functions
	â€¢	Data quality (null checks, type enforcement)

4.2 Notebooks (Spark / Python)
	â€¢	Sample transformations (cleaning, normalization, feature engineering)
	â€¢	Persist as Delta in Silver/Gold
	â€¢	Advanced analytics:
	â€¢	Forecasting / anomaly detection
	â€¢	Classification/regression example
	â€¢	Model tracking (MLflow if using Databricks)

Include snippets or pseudocode; full code can be in an appendix or repo.

â¸»

5. Realâ€‘time Ingestion & Analytics (1â€“2 pages)

5.1 Event Hubs Producers

Simulate producers (Python script or Azure Function) and event schema.

5.2 Stream Analytics Job
	â€¢	Query (e.g., windowed aggregations)
	â€¢	Output sinks (ADLS Gold, Power BI)
	â€¢	Latency and throughput measurements

5.3 Integration with Warehouse/BI

Explain how streaming data lands in near realâ€‘time dashboards alongside batch data.

â¸»

6. Add AI with Cognitive Services (1â€“2 pages)

6.1 Scenario & Model

Choose a prebuilt capability aligned to your data:
	â€¢	Language: sentiment, key phrase extraction, PII detection
	â€¢	Vision: OCR for receipts/images (if applicable)

6.2 Architecture Integration

Where inference runs (notebook, ADF custom activity) and how outputs flow into Silver/Gold/BI.

6.3 Results

Accuracy notes, examples before/after enrichment, and business impact.

â¸»

7. Security, Governance & Compliance (1â€“2 pages)
	â€¢	Identity & Access: Entra ID groups, RBAC, ACLs on ADLS
	â€¢	Secrets: Key Vault for keys/connection strings
	â€¢	Network: Private endpoints (if configured), IP restrictions
	â€¢	Data protection: Encryption at rest/in transit
	â€¢	Posture & alerts: Defender for Cloud recommendations
	â€¢	Lineage & catalog: Purview (optional)

â¸»

8. Deployment Process (1â€“2 pages)

8.1 Approach

Portal / IaC (Bicep/Terraform) / CLI. Describe why chosen.

8.2 Environments & Parameters

Dev/Test/Prod, naming conventions, tags, region choice, scaling tiers.

8.3 Steps
	1.	Provision resource group & storage
	2.	Create ADF/Databricks/Synapse/Event Hubs
	3.	Configure Linked Services, datasets, Spark clusters
	4.	Publish pipelines & notebooks
	5.	Configure Stream Analytics input/output
	6.	Set up Cognitive Services + keys in Key Vault
	7.	Create SQL objects / Lakehouse tables
	8.	Publish Power BI report / Fabric item

Include screenshots of key deployments and success states.

â¸»

9. Testing & Validation (1â€“2 pages)

9.1 Functional Tests
	â€¢	Ingestion completeness, schema validation, DQ checks
	â€¢	Transformation logic correctness
	â€¢	AI inference outputs within expected ranges

9.2 Performance & Reliability
	â€¢	Batch runtimes, streaming endâ€‘toâ€‘end latency
	â€¢	Autoscaling behavior under load

9.3 Monitoring & Troubleshooting
	â€¢	Log Analytics KQL queries (include a few)
	â€¢	ADF pipeline run histories, failure alerts

â¸»

10. Results, Visualizations & Business Insights (1â€“2 pages)

Summarize BI visuals/KPIs and how stakeholders use them. Explain insights generated and decisions enabled.

Insert screenshotsâ€”note they do not count toward page limit.

â¸»

11. Limitations & Future Work (Â½â€“1 page)

Be explicit about tradeâ€‘offs (cost tiers, feature gaps, data quality, model scope) and propose nextâ€‘step roadmap.

â¸»

12. Conclusion (Â½ page)

Short wrapâ€‘up reiterating objectives, architecture, and achieved outcomes.

â¸»

13. References (APA)

Use APA for any sources, SDK docs, and diagrams. References do not count toward page limit.

â¸»

Appendices (optional)
	â€¢	A. Detailed schemas & data dictionary
	â€¢	B. IaC snippets (Bicep/Terraform), ARM exports
	â€¢	C. Notebook code and ADF pipeline JSON
	â€¢	D. Stream Analytics query listing
	â€¢	E. KQL queries for monitoring

â¸»

ðŸ“¸ Evidence & Screenshot Checklist (for Section 10)
	â€¢	Resource group & region
	â€¢	ADLS Gen2 containers (bronze/silver/gold)
	â€¢	ADF pipelines (trigger runs success)
	â€¢	Databricks/Synapse notebook runs
	â€¢	Event Hubs namespace + Stream Analytics job running
	â€¢	Cognitive Services resource + sample inference output
	â€¢	Warehouse tables and successful queries
	â€¢	Power BI / Fabric report with refreshed dataset
	â€¢	Log Analytics workspace with KQL results

â¸»

ðŸ“‘ Formatting Checklist (Assignment Requirements)
	â€¢	15â€“20 pages, doubleâ€‘spaced, Times New Roman 12 pt
	â€¢	Use subheadings; number all pages
	â€¢	Long quotes/lists are singleâ€‘spaced
	â€¢	Clean and professional look
	â€¢	Submit Word file with full name in filename

â¸»

ðŸ§­ How to Use This Template

Replace angleâ€‘bracket sections with your content, and expand each subsection to meet the page count. Keep visuals separate (not counted). Keep a running list of screenshots as you deploy, and paste them into Section 10 and/or Appendices.